{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from fastai.vision import *\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_rows', 70)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the exported, trained model, I will predict the emotions based on the Audio clips where only Sanyam is the speaker, from most of the Episodes.\n",
    "\n",
    "During extraction of the Audio clips, I made some changes compared to the images used for training the model. I removed any axes, ticks, and labels. And removed any possible margins around the image. I also saved these Mel Spectograms as `png`s instead of `jpeg`s like for the training dataset. \n",
    "\n",
    "Minor changes, but for now, this is just piecing things together as quickly as possible and learning along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = ImageList.from_folder(\"/notebooks/storage/ctds_data/audio_files_segments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/storage/ctds_data/audio_files_segments/44_49.png\n"
     ]
    }
   ],
   "source": [
    "print(test_images.items[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(\"/notebooks\", \"model_export\", test=test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2523, 8])\n",
      "tensor([[0.0403, 0.0248, 0.0032, 0.0102, 0.7061, 0.1849, 0.0248, 0.0056],\n",
      "        [0.0450, 0.0080, 0.0035, 0.0071, 0.8305, 0.0239, 0.0754, 0.0065],\n",
      "        [0.0368, 0.0711, 0.0043, 0.0189, 0.3675, 0.3903, 0.1078, 0.0032]])\n"
     ]
    }
   ],
   "source": [
    "preds, y = learn.get_preds(ds_type=DatasetType.Test)\n",
    "\n",
    "print(preds.shape)\n",
    "print(preds[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2523])\n",
      "tensor([4, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(preds, 1)\n",
    "print(labels.shape)\n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "\n",
    "I will create a new csv that stores the predictions corresponding to the clips. Columns for this csv -\n",
    "\n",
    "- episode_num\n",
    "- clip_num\n",
    "- pred_num\n",
    "- pred_label\n",
    "- speaker\n",
    "\n",
    "Using the existing transcripts - \n",
    "\n",
    "- I will also mark the missing values for the clips I didn't include for this as null. \n",
    "- Include timestamps for the clip\n",
    "\n",
    "I am also adding `speaker` so that I can extend this csv file later when I use the rest of the audio clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_check = dict()\n",
    "pattern = r\"segments/(.*).png\"\n",
    "exp = re.compile(pattern)\n",
    "for idx, item in enumerate(test_images.items):\n",
    "    op = exp.findall(str(item))[0]\n",
    "    ep = int(op.split(\"_\")[0])\n",
    "    clip = float(op.split(\"_\")[1])\n",
    "    if ep not in episode_check:\n",
    "        episode_check[ep] = [[clip], [labels[idx]]]\n",
    "    else:\n",
    "        episode_check[ep][0].append(clip)\n",
    "        episode_check[ep][1].append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/notebooks/storage/ctds_data/audio_files/\"\n",
    "path = os.listdir(input_path)\n",
    "data_path = \"/notebooks/storage/ctds_data/Cleaned Subtitles/\"\n",
    "episode_num = []\n",
    "episode_clip = []\n",
    "clip_start = []\n",
    "for file in path:\n",
    "    ep_num = int(file.split('.mp3')[0])\n",
    "\n",
    "    # account for non-interview episodes and missing E4 subtitles and E46\n",
    "    if (ep_num > 45 and ep_num < 55):\n",
    "        continue\n",
    "    elif (ep_num == 4):\n",
    "        episode_num.append(ep_num)\n",
    "        episode_clip.append(np.nan)\n",
    "        clip_start.append(np.nan)\n",
    "        continue\n",
    "    elif ep_num > 54:\n",
    "        ep_num = ep_num - 9\n",
    "    episode_transcript = pd.read_csv(data_path + \"E\" + str(ep_num) + \".csv\")\n",
    "    \n",
    "    count = 0\n",
    "    for idx, time2 in enumerate(episode_transcript[\"Time\"]):\n",
    "        episode_num.append(ep_num)\n",
    "        episode_clip.append(idx)\n",
    "        clip_start.append(time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7650 7650 7650\n"
     ]
    }
   ],
   "source": [
    "print(len(episode_num), len(episode_clip), len(clip_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"episode_num\": episode_num, \"episode_clip\": episode_clip, \"clip_start\": clip_start}\n",
    "predictions = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_num</th>\n",
       "      <th>episode_clip</th>\n",
       "      <th>clip_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3304</th>\n",
       "      <td>75</td>\n",
       "      <td>68.0</td>\n",
       "      <td>32:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>62</td>\n",
       "      <td>60.0</td>\n",
       "      <td>54:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>5</td>\n",
       "      <td>52.0</td>\n",
       "      <td>27:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>37</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>35</td>\n",
       "      <td>52.0</td>\n",
       "      <td>21:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_num  episode_clip clip_start\n",
       "3304           75          68.0      32:35\n",
       "2096           62          60.0      54:21\n",
       "4506            5          52.0      27:18\n",
       "5523           37          69.0    1:00:25\n",
       "6815           35          52.0      21:29"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speaker(df):\n",
    "    for key, val in episode_check.items():\n",
    "        if (df[\"episode_num\"] == key) and (df[\"episode_clip\"] in val[0]):\n",
    "            return \"Sanyam Bhutani\"\n",
    "        \n",
    "    return np.nan\n",
    "            \n",
    "predictions[\"speaker\"] = predictions.apply(speaker, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5188               NaN\n",
       "3350               NaN\n",
       "6088    Sanyam Bhutani\n",
       "5829    Sanyam Bhutani\n",
       "1091               NaN\n",
       "Name: speaker, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['speaker'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_num(df):\n",
    "    for key, val in episode_check.items():\n",
    "        if (df[\"episode_num\"] == key):\n",
    "            for idx, c in enumerate(val[0]):\n",
    "                if df[\"episode_clip\"] == c:\n",
    "                    return int(val[1][idx].item())\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "predictions[\"pred_num\"] = predictions.apply(pred_num, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1121    6.0\n",
       "4178    NaN\n",
       "331     NaN\n",
       "1578    NaN\n",
       "2711    4.0\n",
       "Name: pred_num, dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"pred_num\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {0.0: 'angry', 1.0: 'calm', 2.0: 'disgust', 3.0: 'fearful', 4.0: 'happy', 5.0: 'neutral', 6.0: 'sad', 7.0: 'surprised'}\n",
    "\n",
    "def pred_label(row):\n",
    "    if row and row in data_dict:\n",
    "        return data_dict[row]\n",
    "    return np.nan\n",
    "        \n",
    "predictions[\"pred_label\"] = predictions[\"pred_num\"].apply(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_num</th>\n",
       "      <th>episode_clip</th>\n",
       "      <th>clip_start</th>\n",
       "      <th>speaker</th>\n",
       "      <th>pred_num</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>66</td>\n",
       "      <td>51.0</td>\n",
       "      <td>55:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>15</td>\n",
       "      <td>64.0</td>\n",
       "      <td>47:06</td>\n",
       "      <td>Sanyam Bhutani</td>\n",
       "      <td>4.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0:13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070</th>\n",
       "      <td>36</td>\n",
       "      <td>30.0</td>\n",
       "      <td>13:20</td>\n",
       "      <td>Sanyam Bhutani</td>\n",
       "      <td>4.0</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>48</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11:31</td>\n",
       "      <td>Sanyam Bhutani</td>\n",
       "      <td>6.0</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_num  episode_clip clip_start         speaker  pred_num  \\\n",
       "308            66          51.0      55:12             NaN       NaN   \n",
       "1526           15          64.0      47:06  Sanyam Bhutani       4.0   \n",
       "5535           18           0.0       0:13             NaN       NaN   \n",
       "4070           36          30.0      13:20  Sanyam Bhutani       4.0   \n",
       "2530           48          30.0      11:31  Sanyam Bhutani       6.0   \n",
       "\n",
       "     pred_label  \n",
       "308         NaN  \n",
       "1526      happy  \n",
       "5535        NaN  \n",
       "4070      happy  \n",
       "2530        sad  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think `predictions` is as ready as it can be for the time being. We can move onto analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"/notebooks/storage/ctds_data/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
